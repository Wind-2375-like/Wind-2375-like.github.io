---
layout: post
title: Learning RLHF (PPO) with codes (Huggingface TRL)
date: 2023-09-16 15:50:11
description: Tech essays of Reinforcement Learning from Human Feedback (RLHF) and Proximal Policy Optimization (PPO) with codes in Huggingface TRL.
tags: NLP LLM
categories: TechEssays
thumbnail: https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimg20230913200723.png
images:
  compare: true
  slider: true
---

I briefly learned PPO in the RL and Modern NLP, but I didn't quite grasp it, so over the past couple of days, I glanced at the source code implementation on [TRL - Transformer Reinforcement Learning (huggingface.co)](https://huggingface.co/docs/trl/index) and the paper [[2307.04964] Secrets of RLHF in Large Language Models Part I: PPO (arxiv.org)](https://arxiv.org/abs/2307.04964). I feel much clearer about it now. (Seems like there are quite a few errors in the paper...

First, let's go over the PPO workflow:

<!-- ![](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimg20230913200723.png) -->
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimg20230913200723.png" alt="PPO" width="100%" data-zoomable/>

We start with an initial state where we have an SFT Model and its "clone brother", the RL Model, both referred to as policies. There's also a Value Model. In PPO, there's a process called GAE, which requires the use of Advantage, TD-Error, and Return. Then, finally, through these calculated elements, we derive the PPO-clip Loss, LM Loss, and MSE Loss.

## RL Prerequisites

### Policy Gradient

The core objective of RL is to optimize the RL Model ($$\pi_{\theta}^{\text{RL}}$$). Based on the Policy Gradient and the Log-likelihood Trick, our goal is to maximize the Return under the RL policy (which can initially be understood as Reward; it's actually a multi-time step total discounted reward), using Gradient Ascent:

<!-- ![image.png](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913205439.png) -->
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913205439.png" alt="Policy Gradient" width="100%" data-zoomable/>

Calculating this through Monte-Carlo Sampling leads to high variance, so subtracting the baseline is a better approach. In RL courses, it's known that the baseline is generally the expected reward, which is the V-Value.

Here, I'm not sure why the second and third $$\Phi_{t}​$$ are not the total discounted reward.

### Advantage, V-Value, and Q-Value

It was my first time learning about the relationship between Advantage and Q-Value. After deriving from the Bellman equation for one step, it appears to be correct indeed. I've always learned in class that $$A_{t}=r_{t}+\gamma V(s_{t+1})-V(s_{t})$$:

<!-- ![Advantage formula](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913212410.png) -->
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913212410.png" alt="Advantage formula" width="50%" data-zoomable/>

### A2C (Advantage Actor-Critic)

According to Policy Gradient, we can train both a policy model and a value model. The value model should be as close as possible to the total discounted reward (return), hence the use of MSE loss, while the policy model should aim to maximize the return, hence equation (5) is used:

<!-- ![A2C formula](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913212006.png) -->
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913212006.png" alt="A2C formula" width="100%" data-zoomable/>

### Generalized Advantage Estimation

For multi-time step advantage calculation, see here:

![GAE formula](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913212336.png)

With a small $$k$$, the bias is large. With a large $$k$$, the variance is large. A trade-off is needed, leading to the use of GAE:

<!-- ![GAE extended formula](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913212910.png) ![GAE loop formula](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913212932.png) -->
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913212910.png" alt="GAE extended formula" width="100%" data-zoomable/>
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913212932.png" alt="GAE loop formula" width="100%" data-zoomable/>

This can be simplified into a loop using the algorithm by Qin Jiushao:

<!-- ![GAE simplified loop](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913213139.png) ![](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimg20230913212816.png) -->
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913213139.png" alt="GAE simplified loop" width="100%" data-zoomable/>
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimg20230913212816.png" alt="GAE simplified loop" width="100%" data-zoomable/>

This is how it's implemented in the TRL code.

PPO improves upon the A2C foundation.

## RLHF-PPO

PPO adds constraints on top of A2C to prevent too significant updates in the policy model, ensuring the updated policy model "does not deviate too much" from the previous. The principles have been discussed in RL courses, with lengthy proof formulas, hence not listed here.

### Reward

This is part of the training process. The RLHF's Reward Function is pre-trained:

<!-- ![Training process](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913213739.png) -->
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913213739.png" alt="Training process" width="100%" data-zoomable/>

In PPO, total reward is calculated as follows:

<!-- ![Total reward calculation](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913214122.png) -->
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913214122.png" alt="Total reward calculation" width="100%" data-zoomable/>

**Note**: The KL here and the KL in the later Update Policy section are not exactly the same. Here, KL is calculated between the old policy before each Update Policy and the original SFT model, whereas later, KL is calculated between the policy before and after the update! In the code, the reward is added to the action of generating the last token, with the rest being 0, adhering to the principle of sparsity.

### Update Policy:

**TRPO**

<!-- ![](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimg20230913214624.png) -->
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimg20230913214624.png" alt="TRPO" width="100%" data-zoomable/>

**PPO-CLIP**

<!-- ![PPO-CLIP formula](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913214656.png) -->
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913214656.png" alt="PPO-CLIP formula" width="100%" data-zoomable/>

It's essentially a one-way clip, preventing the model from trying too hard in a good/bad direction when the action is good/bad. Detailed explanation:

<!-- ![](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimg20230913214745.png) -->
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimg20230913214745.png" alt="PPO-CLIP explanation" width="100%" data-zoomable/>

### Update Value

The code also clips the value, then takes the max loss, which is somewhat unusual:

<!-- ![Update Value formula](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913215010.png) -->
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913215010.png" alt="Update Value formula" width="100%" data-zoomable/>

### Mixing Pretraining Gradients

This part was done by InstructGPT and seems not to be implemented in TRL. It corresponds to the LM loss in the workflow:

<!-- ![Mixing Pretraining Gradients](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913215111.png) -->
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913215111.png" alt="Mixing Pretraining Gradients" width="100%" data-zoomable/>

### Pseudocode

The pseudocode and A2C differ slightly from the workflow diagram; in TRL, I did not observe an experience buffer/sampling:

<!-- ![Pseudocode](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913215219.png) -->
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913215219.png" alt="Pseudocode" width="100%" data-zoomable/>

## Code Implementation

### Overall

When using, simply call the `ppo_trainer.step()` function. This function's internal structure is detailed in the TRL GitHub repository:

[trl/trl/trainer/ppo_trainer.py at v0.7.1 · huggingface/trl (github.com)](https://github.com/huggingface/trl/blob/v0.7.1/trl/trainer/ppo_trainer.py#L574)

<!-- ![Overall structure](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914140207.png) -->
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914140207.png" alt="Overall structure" width="100%" data-zoomable/>

Upon creating the trainer, you must pass in an SFT model. The trainer clones it into a reference model (an unupdated SFT) and a policy model, either `AutoModelForCausalLMWithValueHead` or `AutoModelForSeq2SeqLMWithValueHead`. This Value Head acts as the Value Model.

Before entering the step, query and response embeddings are fed in batches, along with the reward for the response. You need to:

```python
for batch in ppo_trainer.dataloader:
	query_tensors = get_query_tensors()
	response_tensors = generate_response_tensors_given_query_with_policy_LM()
	rewards = get_reward_score_for_the_response()
	stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
```

Inside the `step` function, the process begins with two significant operations around lines 665 and 680:

<!-- ![Two significant operations](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914141141.png) -->
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914141141.png" alt="Two significant operations" width="100%" data-zoomable/>

These operations involve model forward passes and reward computations, as highlighted below:

### How to Do Model Forward

The `batched_forward_pass()` function calculates the RL model's policy probabilities $$\pi_{\theta_{\text{old}}}^{\text{RL}}(y_{w}\vert x)$$ (`all_logprobs`), the SFT model's policy probabilities $$\pi^{\text{SFT}}(y_{w}\vert x)$$ (`ref_logprobs`), and the values $$V(x)$$ (`values`).

[trl/trl/trainer/ppo_trainer.py at v0.7.1 · huggingface/trl (github.com)](https://github.com/huggingface/trl/blob/v0.7.1/trl/trainer/ppo_trainer.py#L906)

<!-- ![batched_forward_pass function](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimg20230914142459.png) ![Details of batched_forward_pass](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914142548.png) -->
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimg20230914142459.png" alt="batched_forward_pass function" width="100%" data-zoomable/>
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914142548.png" alt="Details of batched_forward_pass" width="100%" data-zoomable/>

The `logprobs` function calculates the probabilities based on the logits, which represent the scores for all tokens in the vocabulary at each position in the sequence. After applying softmax, the probabilities of the tokens appearing in the response are determined. The comparison involves decoder input from index 1 and decoder output from 0 to the second-to-last token.

<!-- ![Logprobs calculation](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914142734.png) -->
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914142734.png" alt="Logprobs calculation" width="100%" data-zoomable/>

Additionally, a mask (Line 959) is applied to ignore the first token and calculate values only for tokens from index 1 to before the padding token.

In Line 946, `logits, _, values = model(**input_kwargs)`, what is the meaning of `values` here? Does the `forward` function in the HuggingFace have this return value? It turns out that TRL uses `LMWithValueHead` here, letting the model have a `value_head` attribute, which is a linear layer.

### Value Head

The value head is shared with the policy model's underlying parameters but swaps the top layer for a linear model.

[trl/trl/models/modeling_value_head.py at v0.7.1 · huggingface/trl (github.com)](https://github.com/huggingface/trl/blob/v0.7.1/trl/models/modeling_value_head.py#L260)

<!-- ![Value Head structure](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913102810.png) -->
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913102810.png" alt="Value Head structure" width="100%" data-zoomable/>

The `self.v_head()` function requires the last hidden state to predict the value.

<!-- ![Value Head operation](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913103215.png)  -->
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913103215.png" alt="Value Head operation" width="100%" data-zoomable/>

<!-- ![Further details on Value Head](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913103109.png) -->
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913103109.png" alt="Further details on Value Head" width="100%" data-zoomable/>

### Reward

The `compute_reward()` function is crucial for calculating the rewards based on the policy probabilities and SFT probabilities computed earlier.

<!-- ![compute_reward function](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914143435.png) -->
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914143435.png" alt="compute_reward function" width="100%" data-zoomable/>

The rewards are calculated for each token treated as an action, computing $$r−\beta×KL$$, with $$r$$ having a value only for the last token, as per design.

### Advantage (GAE)

<!-- ![Advantage (GAE) calculation](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914144039.png) -->
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914144039.png" alt="Advantage (GAE) calculation" width="100%" data-zoomable/>

The Generalized Advantage Estimation (GAE) calculates values, advantages, and returns, incorporating a mask for calculation.

<!-- ![Details on GAE calculation](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914144124.png) -->
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914144124.png" alt="Details on GAE calculation" width="100%" data-zoomable/>

### PPO Update

All data is stored as old outputs before iterating over minibatches for `self.config.ppo_epochs` epochs defined by `self.config.ppo_epochs`.

<!-- ![PPO Update preparation](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914144241.png) ![Minibatch iteration](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914144403.png) -->
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914144241.png" alt="PPO Update preparation" width="100%" data-zoomable/>
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914144403.png" alt="Minibatch iteration" width="100%" data-zoomable/>

The minibatch dictionary `mini_batch_dict` calculates the information before the PPO update.

<!-- ![Minibatch information](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimg20230914144537.png) -->
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimg20230914144537.png" alt="Minibatch information" width="100%" data-zoomable/>

The training of minibatches combines the Policy and Value losses for joint updates.

<!-- ![Training minibatches](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914144606.png) ![Combined loss update](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimg20230914144700.png) -->
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914144606.png" alt="Training minibatches" width="100%" data-zoomable/>
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimg20230914144700.png" alt="Combined loss update" width="100%" data-zoomable/>

The loss function is specially tailored, with the value loss being clipped and averaged over unmasked values, while the policy gradient loss remains standard.

<!-- ![Loss function details](https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914145020.png) -->
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914145020.png" alt="Loss function details" width="100%" data-zoomable/>

This comprehensive overview explains how the `step` function integrates various components of the PPO algorithm, from calculating probabilities and values to updating the policy and value models based on computed rewards and advantages, ultimately refining the RL model's behavior towards desired outcomes.