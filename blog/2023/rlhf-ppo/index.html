<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Learning RLHF (PPO) with codes (Huggingface TRL) | Yiyang Feng </title> <meta name="author" content="Yiyang Feng"> <meta name="description" content="Tech essays of Reinforcement Learning from Human Feedback (RLHF) and Proximal Policy Optimization (PPO) with codes in Huggingface TRL."> <meta name="keywords" content="NLP, ML, AI"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?25191efbaa8819fa48bb17b7177a0954"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://wind-2375-like.github.io/blog/2023/rlhf-ppo/"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8.0.6/dist/styles.min.css"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-bundle.min.css" integrity="sha256-yUoNxsvX+Vo8Trj3lZ/Y5ZBf8HlBFsB6Xwm7rH75/9E=" crossorigin="anonymous"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yiyang</span> Feng </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Learning RLHF (PPO) with codes (Huggingface TRL)</h1> <p class="post-meta"> September 16, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> LLM</a>     ·   <a href="/blog/category/techessays"> <i class="fa-solid fa-tag fa-sm"></i> TechEssays</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>I briefly learned PPO in the RL and Modern NLP, but I didn’t quite grasp it, so over the past couple of days, I glanced at the source code implementation on <a href="https://huggingface.co/docs/trl/index" rel="external nofollow noopener" target="_blank">TRL - Transformer Reinforcement Learning (huggingface.co)</a> and the paper <a href="https://arxiv.org/abs/2307.04964" rel="external nofollow noopener" target="_blank">[2307.04964] Secrets of RLHF in Large Language Models Part I: PPO (arxiv.org)</a>. I feel much clearer about it now. (Seems like there are quite a few errors in the paper…</p> <p>First, let’s go over the PPO workflow:</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimg20230913200723.png" alt="PPO" width="100%" data-zoomable=""></p> <p>We start with an initial state where we have an SFT Model and its “clone brother”, the RL Model, both referred to as policies. There’s also a Value Model. In PPO, there’s a process called GAE, which requires the use of Advantage, TD-Error, and Return. Then, finally, through these calculated elements, we derive the PPO-clip Loss, LM Loss, and MSE Loss.</p> <h2 id="rl-prerequisites">RL Prerequisites</h2> <h3 id="policy-gradient">Policy Gradient</h3> <p>The core objective of RL is to optimize the RL Model (\(\pi_{\theta}^{\text{RL}}\)). Based on the Policy Gradient and the Log-likelihood Trick, our goal is to maximize the Return under the RL policy (which can initially be understood as Reward; it’s actually a multi-time step total discounted reward), using Gradient Ascent:</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913205439.png" alt="Policy Gradient" width="100%" data-zoomable=""></p> <p>Calculating this through Monte-Carlo Sampling leads to high variance, so subtracting the baseline is a better approach. In RL courses, it’s known that the baseline is generally the expected reward, which is the V-Value.</p> <p>Here, I’m not sure why the second and third \(\Phi_{t}​\) are not the total discounted reward.</p> <h3 id="advantage-v-value-and-q-value">Advantage, V-Value, and Q-Value</h3> <p>It was my first time learning about the relationship between Advantage and Q-Value. After deriving from the Bellman equation for one step, it appears to be correct indeed. I’ve always learned in class that \(A_{t}=r_{t}+\gamma V(s_{t+1})-V(s_{t})\):</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913212410.png" alt="Advantage formula" width="50%" data-zoomable=""></p> <h3 id="a2c-advantage-actor-critic">A2C (Advantage Actor-Critic)</h3> <p>According to Policy Gradient, we can train both a policy model and a value model. The value model should be as close as possible to the total discounted reward (return), hence the use of MSE loss, while the policy model should aim to maximize the return, hence equation (5) is used:</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913212006.png" alt="A2C formula" width="100%" data-zoomable=""></p> <h3 id="generalized-advantage-estimation">Generalized Advantage Estimation</h3> <p>For multi-time step advantage calculation, see here:</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913212336.png" alt="GAE formula"></p> <p>With a small \(k\), the bias is large. With a large \(k\), the variance is large. A trade-off is needed, leading to the use of GAE:</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913212910.png" alt="GAE extended formula" width="100%" data-zoomable=""> <img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913212932.png" alt="GAE loop formula" width="100%" data-zoomable=""></p> <p>This can be simplified into a loop using the algorithm by Qin Jiushao:</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913213139.png" alt="GAE simplified loop" width="100%" data-zoomable=""> <img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimg20230913212816.png" alt="GAE simplified loop" width="100%" data-zoomable=""></p> <p>This is how it’s implemented in the TRL code.</p> <p>PPO improves upon the A2C foundation.</p> <h2 id="rlhf-ppo">RLHF-PPO</h2> <p>PPO adds constraints on top of A2C to prevent too significant updates in the policy model, ensuring the updated policy model “does not deviate too much” from the previous. The principles have been discussed in RL courses, with lengthy proof formulas, hence not listed here.</p> <h3 id="reward">Reward</h3> <p>This is part of the training process. The RLHF’s Reward Function is pre-trained:</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913213739.png" alt="Training process" width="100%" data-zoomable=""></p> <p>In PPO, total reward is calculated as follows:</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913214122.png" alt="Total reward calculation" width="100%" data-zoomable=""></p> <p><strong>Note</strong>: The KL here and the KL in the later Update Policy section are not exactly the same. Here, KL is calculated between the old policy before each Update Policy and the original SFT model, whereas later, KL is calculated between the policy before and after the update! In the code, the reward is added to the action of generating the last token, with the rest being 0, adhering to the principle of sparsity.</p> <h3 id="update-policy">Update Policy:</h3> <p><strong>TRPO</strong></p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimg20230913214624.png" alt="TRPO" width="100%" data-zoomable=""></p> <p><strong>PPO-CLIP</strong></p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913214656.png" alt="PPO-CLIP formula" width="100%" data-zoomable=""></p> <p>It’s essentially a one-way clip, preventing the model from trying too hard in a good/bad direction when the action is good/bad. Detailed explanation:</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimg20230913214745.png" alt="PPO-CLIP explanation" width="100%" data-zoomable=""></p> <h3 id="update-value">Update Value</h3> <p>The code also clips the value, then takes the max loss, which is somewhat unusual:</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913215010.png" alt="Update Value formula" width="100%" data-zoomable=""></p> <h3 id="mixing-pretraining-gradients">Mixing Pretraining Gradients</h3> <p>This part was done by InstructGPT and seems not to be implemented in TRL. It corresponds to the LM loss in the workflow:</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913215111.png" alt="Mixing Pretraining Gradients" width="100%" data-zoomable=""></p> <h3 id="pseudocode">Pseudocode</h3> <p>The pseudocode and A2C differ slightly from the workflow diagram; in TRL, I did not observe an experience buffer/sampling:</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913215219.png" alt="Pseudocode" width="100%" data-zoomable=""></p> <h2 id="code-implementation">Code Implementation</h2> <h3 id="overall">Overall</h3> <p>When using, simply call the <code class="language-plaintext highlighter-rouge">ppo_trainer.step()</code> function. This function’s internal structure is detailed in the TRL GitHub repository:</p> <p><a href="https://github.com/huggingface/trl/blob/v0.7.1/trl/trainer/ppo_trainer.py#L574" rel="external nofollow noopener" target="_blank">trl/trl/trainer/ppo_trainer.py at v0.7.1 · huggingface/trl (github.com)</a></p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914140207.png" alt="Overall structure" width="100%" data-zoomable=""></p> <p>Upon creating the trainer, you must pass in an SFT model. The trainer clones it into a reference model (an unupdated SFT) and a policy model, either <code class="language-plaintext highlighter-rouge">AutoModelForCausalLMWithValueHead</code> or <code class="language-plaintext highlighter-rouge">AutoModelForSeq2SeqLMWithValueHead</code>. This Value Head acts as the Value Model.</p> <p>Before entering the step, query and response embeddings are fed in batches, along with the reward for the response. You need to:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">ppo_trainer</span><span class="p">.</span><span class="n">dataloader</span><span class="p">:</span>
	<span class="n">query_tensors</span> <span class="o">=</span> <span class="nf">get_query_tensors</span><span class="p">()</span>
	<span class="n">response_tensors</span> <span class="o">=</span> <span class="nf">generate_response_tensors_given_query_with_policy_LM</span><span class="p">()</span>
	<span class="n">rewards</span> <span class="o">=</span> <span class="nf">get_reward_score_for_the_response</span><span class="p">()</span>
	<span class="n">stats</span> <span class="o">=</span> <span class="n">ppo_trainer</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">query_tensors</span><span class="p">,</span> <span class="n">response_tensors</span><span class="p">,</span> <span class="n">rewards</span><span class="p">)</span>
</code></pre></div></div> <p>Inside the <code class="language-plaintext highlighter-rouge">step</code> function, the process begins with two significant operations around lines 665 and 680:</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914141141.png" alt="Two significant operations" width="100%" data-zoomable=""></p> <p>These operations involve model forward passes and reward computations, as highlighted below:</p> <h3 id="how-to-do-model-forward">How to Do Model Forward</h3> <p>The <code class="language-plaintext highlighter-rouge">batched_forward_pass()</code> function calculates the RL model’s policy probabilities \(\pi_{\theta_{\text{old}}}^{\text{RL}}(y_{w}\vert x)\) (<code class="language-plaintext highlighter-rouge">all_logprobs</code>), the SFT model’s policy probabilities \(\pi^{\text{SFT}}(y_{w}\vert x)\) (<code class="language-plaintext highlighter-rouge">ref_logprobs</code>), and the values \(V(x)\) (<code class="language-plaintext highlighter-rouge">values</code>).</p> <p><a href="https://github.com/huggingface/trl/blob/v0.7.1/trl/trainer/ppo_trainer.py#L906" rel="external nofollow noopener" target="_blank">trl/trl/trainer/ppo_trainer.py at v0.7.1 · huggingface/trl (github.com)</a></p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimg20230914142459.png" alt="batched_forward_pass function" width="100%" data-zoomable=""> <img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914142548.png" alt="Details of batched_forward_pass" width="100%" data-zoomable=""></p> <p>The <code class="language-plaintext highlighter-rouge">logprobs</code> function calculates the probabilities based on the logits, which represent the scores for all tokens in the vocabulary at each position in the sequence. After applying softmax, the probabilities of the tokens appearing in the response are determined. The comparison involves decoder input from index 1 and decoder output from 0 to the second-to-last token.</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914142734.png" alt="Logprobs calculation" width="100%" data-zoomable=""></p> <p>Additionally, a mask (Line 959) is applied to ignore the first token and calculate values only for tokens from index 1 to before the padding token.</p> <p>In Line 946, <code class="language-plaintext highlighter-rouge">logits, _, values = model(**input_kwargs)</code>, what is the meaning of <code class="language-plaintext highlighter-rouge">values</code> here? Does the <code class="language-plaintext highlighter-rouge">forward</code> function in the HuggingFace have this return value? It turns out that TRL uses <code class="language-plaintext highlighter-rouge">LMWithValueHead</code> here, letting the model have a <code class="language-plaintext highlighter-rouge">value_head</code> attribute, which is a linear layer.</p> <h3 id="value-head">Value Head</h3> <p>The value head is shared with the policy model’s underlying parameters but swaps the top layer for a linear model.</p> <p><a href="https://github.com/huggingface/trl/blob/v0.7.1/trl/models/modeling_value_head.py#L260" rel="external nofollow noopener" target="_blank">trl/trl/models/modeling_value_head.py at v0.7.1 · huggingface/trl (github.com)</a></p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913102810.png" alt="Value Head structure" width="100%" data-zoomable=""></p> <p>The <code class="language-plaintext highlighter-rouge">self.v_head()</code> function requires the last hidden state to predict the value.</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913103215.png" alt="Value Head operation" width="100%" data-zoomable=""></p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913103109.png" alt="Further details on Value Head" width="100%" data-zoomable=""></p> <h3 id="reward-1">Reward</h3> <p>The <code class="language-plaintext highlighter-rouge">compute_reward()</code> function is crucial for calculating the rewards based on the policy probabilities and SFT probabilities computed earlier.</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914143435.png" alt="compute_reward function" width="100%" data-zoomable=""></p> <p>The rewards are calculated for each token treated as an action, computing \(r−\beta×KL\), with \(r\) having a value only for the last token, as per design.</p> <h3 id="advantage-gae">Advantage (GAE)</h3> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914144039.png" alt="Advantage (GAE) calculation" width="100%" data-zoomable=""></p> <p>The Generalized Advantage Estimation (GAE) calculates values, advantages, and returns, incorporating a mask for calculation.</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914144124.png" alt="Details on GAE calculation" width="100%" data-zoomable=""></p> <h3 id="ppo-update">PPO Update</h3> <p>All data is stored as old outputs before iterating over minibatches for <code class="language-plaintext highlighter-rouge">self.config.ppo_epochs</code> epochs defined by <code class="language-plaintext highlighter-rouge">self.config.ppo_epochs</code>.</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914144241.png" alt="PPO Update preparation" width="100%" data-zoomable=""> <img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914144403.png" alt="Minibatch iteration" width="100%" data-zoomable=""></p> <p>The minibatch dictionary <code class="language-plaintext highlighter-rouge">mini_batch_dict</code> calculates the information before the PPO update.</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimg20230914144537.png" alt="Minibatch information" width="100%" data-zoomable=""></p> <p>The training of minibatches combines the Policy and Value losses for joint updates.</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914144606.png" alt="Training minibatches" width="100%" data-zoomable=""> <img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimg20230914144700.png" alt="Combined loss update" width="100%" data-zoomable=""></p> <p>The loss function is specially tailored, with the value loss being clipped and averaged over unmasked values, while the policy gradient loss remains standard.</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914145020.png" alt="Loss function details" width="100%" data-zoomable=""></p> <p>This comprehensive overview explains how the <code class="language-plaintext highlighter-rouge">step</code> function integrates various components of the PPO algorithm, from calculating probabilities and values to updating the policy and value models based on computed rewards and advantages, ultimately refining the RL model’s behavior towards desired outcomes.</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Yiyang Feng. 竹杖芒鞋轻胜马，谁怕？一蓑烟雨任平生。 </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script defer src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8.0.6/dist/index.min.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-element-bundle.min.js" integrity="sha256-BPrwikijIybg9OQC5SYFFqhBjERYOn97tCureFgYH1E=" crossorigin="anonymous"></script> </body> </html>