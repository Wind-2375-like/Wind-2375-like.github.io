<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Reading Notes of How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources | Yiyang Feng </title> <meta name="author" content="Yiyang Feng"> <meta name="description" content="Reading notes of Yao's notes of " how does gpt obtain its ability tracing emergent abilities of language models to their sources> <meta name="keywords" content="NLP, ML, AI"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?25191efbaa8819fa48bb17b7177a0954"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://wind-2375-like.github.io/blog/2023/reading-notes-chatgpt/"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8.0.6/dist/styles.min.css"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-bundle.min.css" integrity="sha256-yUoNxsvX+Vo8Trj3lZ/Y5ZBf8HlBFsB6Xwm7rH75/9E=" crossorigin="anonymous"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yiyang</span> Feng </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Reading Notes of How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources</h1> <p class="post-meta"> February 19, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> LLM</a>     ·   <a href="/blog/category/readingnotes"> <i class="fa-solid fa-tag fa-sm"></i> ReadingNotes</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Reading notes of Yao’s notes of <a href="https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1" rel="external nofollow noopener" target="_blank">How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources (notion.site)</a>.</p> <p><strong>Keywords:</strong> code and instruction tuning; trade of between in-context learning and instruction tuning; unlock or inject the abilities; instruction-tuning, supervised instruction-tuning, and RLHF instruction-tuning; knowledge and reasoning.</p> <h2 id="reference">Reference</h2> <blockquote> <p>Fu, Yao; Peng, Hao and Khot, Tushar. (Dec 2022). How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources. Yao Fu’s Notion. https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1</p> </blockquote> <h2 id="road-map">Road Map</h2> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimage-20230218225348976.png" alt="image-20230218225348976"></p> <p>Note: The base model for code-davinci-002 is highly likely not be the initial GPT-3 davinci model.</p> <h2 id="summary">Summary</h2> <p>The authors’ conclusion:</p> <ul> <li>The <strong>language generation ability</strong> + <strong>basic world knowledge</strong> + <strong>in-context learning</strong> are from pretraining (<code class="language-plaintext highlighter-rouge">davinci</code>)</li> <li>The ability to <strong>store a large amount of ==knowledge==</strong> is from the 175B scale.</li> <li>The ability to <strong>follow instructions</strong> and <strong>generalizing to new tasks</strong> are from scaling instruction tuning (<code class="language-plaintext highlighter-rouge">davinci-instruct-beta</code>)</li> <li>The ability to perform <strong>complex ==reasoning==</strong> is likely to be from training on code (<code class="language-plaintext highlighter-rouge">code-davinci-002</code>)</li> <li>The ability to <strong>generate neutral, objective, safe, and informative answers</strong> are from alignment with human. Specifically: <ul> <li>If supervised tuning, the resulting model is <code class="language-plaintext highlighter-rouge">text-davinci-002</code> </li> <li>If RLHF, the resulting model is <code class="language-plaintext highlighter-rouge">text-davinci-003</code> </li> <li>Either supervised or RLHF, the models cannot outperform <code class="language-plaintext highlighter-rouge">code-davinci-002</code> on many tasks, which is called the <em>alignment tax</em>. (RLHF on 003 just recovers the in-context learning ability.)</li> </ul> </li> <li>The <strong>dialog ability</strong> is also from RLHF (<code class="language-plaintext highlighter-rouge">ChatGPT</code>), specifically it tradeoffs in-context learning for: <ul> <li>Modeling dialog history</li> <li>Increased informativeness</li> <li>Rejecting questions outside the model’s knowledge scope</li> </ul> </li> </ul> <table> <thead> <tr> <th>Ability</th> <th>OpenAI Model</th> <th>Training Method</th> <th>OpenAI API</th> <th>OpenAI Paper</th> <th>Open Source Approximate</th> </tr> </thead> <tbody> <tr> <td>GPT-3 Series</td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td>Generation <br>+ World Knowledge <br>+ In-context Learning</td> <td>GPT-3 Initial<br><br><strong>Many abilities already within this model, although superficially weak</strong> </td> <td>Language Modeling</td> <td>Davinci</td> <td><a href="https://arxiv.org/abs/2005.14165" rel="external nofollow noopener" target="_blank">GPT 3 Paper</a></td> <td> <a href="https://arxiv.org/abs/2205.01068" rel="external nofollow noopener" target="_blank">Meta OPT</a><br><a href="https://arxiv.org/abs/2211.05100" rel="external nofollow noopener" target="_blank">BLOOM</a><br><a href="https://arxiv.org/abs/2211.09085" rel="external nofollow noopener" target="_blank">Galactica</a> </td> </tr> <tr> <td>+ Follow Human Instruction<br>+ Generalize to unseen task (free lunch of scaling instructions)</td> <td>Instruct-GPT initial</td> <td>Instruction Tuning</td> <td>Davinci-Instruct-Beta</td> <td><a href="https://arxiv.org/abs/2203.02155" rel="external nofollow noopener" target="_blank">Instruct-GPT paper</a></td> <td> <a href="https://arxiv.org/abs/2110.08207" rel="external nofollow noopener" target="_blank">T0 paper</a><br><a href="https://arxiv.org/abs/2109.01652" rel="external nofollow noopener" target="_blank">Google FLAN paper</a><br><a href="https://arxiv.org/abs/2210.11416" rel="external nofollow noopener" target="_blank">Google FLAN paper</a> </td> </tr> <tr> <td>+ Code Understanding<br>+ Code Generation</td> <td>Codex initial</td> <td>Training on Code</td> <td>Code-Cushman-001</td> <td><a href="https://arxiv.org/abs/2107.03374" rel="external nofollow noopener" target="_blank">Codex Paper</a></td> <td><a href="https://arxiv.org/abs/2203.13474" rel="external nofollow noopener" target="_blank">Salesforce CodeGen</a></td> </tr> <tr> <td>GPT-3.5 Series</td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td>++ Code Understanding<br>++ Code Generation<br>++ Complex Reasoning / Chain of Thought (<em>why</em>?)<br>+ Long-term dependency (<em>probably</em>)</td> <td>Current Codex<br><br><strong>Strongest model in GPT3.5 Series</strong> </td> <td>Training on text + code <br>Tuning on instructions</td> <td>Code-Davinci-002</td> <td><a href="https://arxiv.org/abs/2107.03374" rel="external nofollow noopener" target="_blank">Codex Paper</a></td> <td>??</td> </tr> <tr> <td>++ Follow Human Instruction<br>- In-context learning<br>- Reasoning<br>++ Zero-shot generation</td> <td>Instruct-GPT supervised<br><br><strong>Trade in-context learning for zero-shot generation</strong> </td> <td>Supervised instruction tuning</td> <td>Text-Davinci-002</td> <td> <a href="https://arxiv.org/abs/2203.02155" rel="external nofollow noopener" target="_blank">Instruct-GPT paper</a>, supervised part</td> <td> <a href="https://arxiv.org/abs/2110.08207" rel="external nofollow noopener" target="_blank">T0 paper</a><br><a href="https://arxiv.org/abs/2109.01652" rel="external nofollow noopener" target="_blank">Google FLAN paper</a><br><a href="https://arxiv.org/abs/2210.11416" rel="external nofollow noopener" target="_blank">Google FLAN paper</a> </td> </tr> <tr> <td>+ Follow human value<br>+ More detailed generation<br>+ In-context learning<br>+ Zero-shot generation</td> <td>Instruct-GPT RLHF<br><br><strong>More aligned than 002, less performance loss</strong> </td> <td>Instruction tuning w. RLHF</td> <td>Text-Davinci-003</td> <td>Instruct-GPT paper, RLHF part <a href="https://arxiv.org/abs/2009.01325" rel="external nofollow noopener" target="_blank">Summarization .w human feedback</a> </td> <td> <a href="https://arxiv.org/abs/2209.14375" rel="external nofollow noopener" target="_blank">DeepMind Sparrow paper</a><br><a href="https://arxiv.org/abs/2210.01241" rel="external nofollow noopener" target="_blank">AI2 RL4LMs</a> </td> </tr> <tr> <td>++ Follow human value<br>++ More detailed generation<br>++ Reject questions beyond its knowledge (why?)<br>++ Model dialog context<br>– In-context learning</td> <td>ChatGPT<br><br><strong>Trade in-context learning for dialog history modeling</strong> </td> <td>Tuning on dialog w. RLHF</td> <td>-</td> <td>-</td> <td> <a href="https://arxiv.org/abs/2209.14375" rel="external nofollow noopener" target="_blank">DeepMind Sparrow paper</a><br><a href="https://arxiv.org/abs/2210.01241" rel="external nofollow noopener" target="_blank">AI2 RL4LMs</a> </td> </tr> </tbody> </table> <h2 id="limitations">Limitations</h2> <ol> <li> <strong>On-the-fly overwriting the model’s belief</strong>: when the model expresses its belief in something, it might be hard to correct it when the belief is wrong. There seems to be a hierarchy of how strong the belief is, which means that there exists a list of very strong core belief. ==<strong>It is extremely important to ensure such core belief should be absolutely 100% aligned with human.</strong>==</li> <li> <strong>Formal reasoning</strong>: the GPT-3.5 series cannot do reasoning within formal, strict systems like math or first-order logic. <ol> <li>The word “reasoning” is less well-defined. Yet if we view there is a spectrum of ambiguity like (a) very ambiguous, no reasoning; (b) mixture of logic and ambiguous statements; (c). no ambiguity has to be very rigorous, then,</li> <li>The model can do very well on type (b) reasoning with ambiguity.</li> <li>The model cannot do type (c) reasoning, <strong>yet whether such reasoning should be done by a language model or a symbolic system is up for discussion.</strong> </li> <li>Update: <a href="https://writings.stephenwolfram.com/2023/01/wolframalpha-as-the-way-to-bring-computational-knowledge-superpowers-to-chatgpt/" rel="external nofollow noopener" target="_blank">Wolfram Alpha as the Way to Bring Computational Knowledge Superpowers to ChatGPT—Stephen Wolfram Writings</a> </li> </ol> </li> <li> <strong>Retrieval from the Internet</strong>: the GPT-3.5 series cannot directly search the internet (for now) <ol> <li>Likely tested within OpenAI: <a href="https://openai.com/blog/webgpt/" rel="external nofollow noopener" target="_blank">WebGPT: Improving the Factual Accuracy of Language Models through Web Browsing (openai.com)</a> </li> <li> <strong>Separate the knowledge and reasoning</strong>: it would be ideal if we could offload the knowledge part to the outside retrieval system and let the language model only focus on reasoning.</li> <li> <strong>Combining LLMs (reasoning) and search (knowledge) is a good direction</strong>: LLMs are good for reasoning, not for knowledge. The knowledge within LLMs are unreliable and cannot be verified. On the other hand, the knowledge from search engine is orders or magnitude larger than LLM’s internal knowledge, can one can easily verify credibility of search results by checking the source. <ol> <li>Retrieval-augmented models?</li> <li>Reduce the model size? (175B storing unreliable knowledge -&gt; search engines)</li> </ol> </li> </ol> </li> </ol> <h2 id="questions-to-be-answered">Questions to be answered</h2> <ol> <li>How does the GPT-3.5 acquire the reasoning ability (CoT)? (Likely because of code training according to the authors).</li> <li>Which kinds of abilities are unlocked/injected by what means<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>? The authors’ hypothesis (according to scale): <ol> <li>Abilities from code training: injection</li> <li>Abilities from instruction tuning: unlock</li> </ol> </li> </ol> <h2 id="what-i-need-to-further-know-about">What I need to further know about</h2> <ul> <li>instruction-tuning? supervised instruction-tuning? RLHF instruction-tuning?</li> <li>code training?</li> </ul> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>“by what means” is hypothesized in the <a href="##summary">summary</a> section but some of them are not proved. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Yiyang Feng. 竹杖芒鞋轻胜马，谁怕？一蓑烟雨任平生。 </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script defer src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8.0.6/dist/index.min.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-element-bundle.min.js" integrity="sha256-BPrwikijIybg9OQC5SYFFqhBjERYOn97tCureFgYH1E=" crossorigin="anonymous"></script> </body> </html>