<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://wind-2375-like.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://wind-2375-like.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-08-08T11:45:36+00:00</updated><id>https://wind-2375-like.github.io/feed.xml</id><title type="html">blank</title><subtitle>MSc Student in Computer Science @ EPFL </subtitle><entry><title type="html">Learning RLHF (PPO) with codes (Huggingface TRL)</title><link href="https://wind-2375-like.github.io/blog/2023/rlhf-ppo/" rel="alternate" type="text/html" title="Learning RLHF (PPO) with codes (Huggingface TRL)"/><published>2023-09-16T15:50:11+00:00</published><updated>2023-09-16T15:50:11+00:00</updated><id>https://wind-2375-like.github.io/blog/2023/rlhf-ppo</id><content type="html" xml:base="https://wind-2375-like.github.io/blog/2023/rlhf-ppo/"><![CDATA[<p>I briefly learned PPO in the RL and Modern NLP, but I didn’t quite grasp it, so over the past couple of days, I glanced at the source code implementation on <a href="https://huggingface.co/docs/trl/index">TRL - Transformer Reinforcement Learning (huggingface.co)</a> and the paper <a href="https://arxiv.org/abs/2307.04964">[2307.04964] Secrets of RLHF in Large Language Models Part I: PPO (arxiv.org)</a>. I feel much clearer about it now. (Seems like there are quite a few errors in the paper…</p> <p>First, let’s go over the PPO workflow:</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimg20230913200723.png" alt="PPO" width="100%" data-zoomable=""/></p> <p>We start with an initial state where we have an SFT Model and its “clone brother”, the RL Model, both referred to as policies. There’s also a Value Model. In PPO, there’s a process called GAE, which requires the use of Advantage, TD-Error, and Return. Then, finally, through these calculated elements, we derive the PPO-clip Loss, LM Loss, and MSE Loss.</p> <h2 id="rl-prerequisites">RL Prerequisites</h2> <h3 id="policy-gradient">Policy Gradient</h3> <p>The core objective of RL is to optimize the RL Model (\(\pi_{\theta}^{\text{RL}}\)). Based on the Policy Gradient and the Log-likelihood Trick, our goal is to maximize the Return under the RL policy (which can initially be understood as Reward; it’s actually a multi-time step total discounted reward), using Gradient Ascent:</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913205439.png" alt="Policy Gradient" width="100%" data-zoomable=""/></p> <p>Calculating this through Monte-Carlo Sampling leads to high variance, so subtracting the baseline is a better approach. In RL courses, it’s known that the baseline is generally the expected reward, which is the V-Value.</p> <p>Here, I’m not sure why the second and third \(\Phi_{t}​\) are not the total discounted reward.</p> <h3 id="advantage-v-value-and-q-value">Advantage, V-Value, and Q-Value</h3> <p>It was my first time learning about the relationship between Advantage and Q-Value. After deriving from the Bellman equation for one step, it appears to be correct indeed. I’ve always learned in class that \(A_{t}=r_{t}+\gamma V(s_{t+1})-V(s_{t})\):</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913212410.png" alt="Advantage formula" width="50%" data-zoomable=""/></p> <h3 id="a2c-advantage-actor-critic">A2C (Advantage Actor-Critic)</h3> <p>According to Policy Gradient, we can train both a policy model and a value model. The value model should be as close as possible to the total discounted reward (return), hence the use of MSE loss, while the policy model should aim to maximize the return, hence equation (5) is used:</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913212006.png" alt="A2C formula" width="100%" data-zoomable=""/></p> <h3 id="generalized-advantage-estimation">Generalized Advantage Estimation</h3> <p>For multi-time step advantage calculation, see here:</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913212336.png" alt="GAE formula"/></p> <p>With a small \(k\), the bias is large. With a large \(k\), the variance is large. A trade-off is needed, leading to the use of GAE:</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913212910.png" alt="GAE extended formula" width="100%" data-zoomable=""/> <img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913212932.png" alt="GAE loop formula" width="100%" data-zoomable=""/></p> <p>This can be simplified into a loop using the algorithm by Qin Jiushao:</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913213139.png" alt="GAE simplified loop" width="100%" data-zoomable=""/> <img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimg20230913212816.png" alt="GAE simplified loop" width="100%" data-zoomable=""/></p> <p>This is how it’s implemented in the TRL code.</p> <p>PPO improves upon the A2C foundation.</p> <h2 id="rlhf-ppo">RLHF-PPO</h2> <p>PPO adds constraints on top of A2C to prevent too significant updates in the policy model, ensuring the updated policy model “does not deviate too much” from the previous. The principles have been discussed in RL courses, with lengthy proof formulas, hence not listed here.</p> <h3 id="reward">Reward</h3> <p>This is part of the training process. The RLHF’s Reward Function is pre-trained:</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913213739.png" alt="Training process" width="100%" data-zoomable=""/></p> <p>In PPO, total reward is calculated as follows:</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913214122.png" alt="Total reward calculation" width="100%" data-zoomable=""/></p> <p><strong>Note</strong>: The KL here and the KL in the later Update Policy section are not exactly the same. Here, KL is calculated between the old policy before each Update Policy and the original SFT model, whereas later, KL is calculated between the policy before and after the update! In the code, the reward is added to the action of generating the last token, with the rest being 0, adhering to the principle of sparsity.</p> <h3 id="update-policy">Update Policy:</h3> <p><strong>TRPO</strong></p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimg20230913214624.png" alt="TRPO" width="100%" data-zoomable=""/></p> <p><strong>PPO-CLIP</strong></p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913214656.png" alt="PPO-CLIP formula" width="100%" data-zoomable=""/></p> <p>It’s essentially a one-way clip, preventing the model from trying too hard in a good/bad direction when the action is good/bad. Detailed explanation:</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimg20230913214745.png" alt="PPO-CLIP explanation" width="100%" data-zoomable=""/></p> <h3 id="update-value">Update Value</h3> <p>The code also clips the value, then takes the max loss, which is somewhat unusual:</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913215010.png" alt="Update Value formula" width="100%" data-zoomable=""/></p> <h3 id="mixing-pretraining-gradients">Mixing Pretraining Gradients</h3> <p>This part was done by InstructGPT and seems not to be implemented in TRL. It corresponds to the LM loss in the workflow:</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913215111.png" alt="Mixing Pretraining Gradients" width="100%" data-zoomable=""/></p> <h3 id="pseudocode">Pseudocode</h3> <p>The pseudocode and A2C differ slightly from the workflow diagram; in TRL, I did not observe an experience buffer/sampling:</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913215219.png" alt="Pseudocode" width="100%" data-zoomable=""/></p> <h2 id="code-implementation">Code Implementation</h2> <h3 id="overall">Overall</h3> <p>When using, simply call the <code class="language-plaintext highlighter-rouge">ppo_trainer.step()</code> function. This function’s internal structure is detailed in the TRL GitHub repository:</p> <p><a href="https://github.com/huggingface/trl/blob/v0.7.1/trl/trainer/ppo_trainer.py#L574">trl/trl/trainer/ppo_trainer.py at v0.7.1 · huggingface/trl (github.com)</a></p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914140207.png" alt="Overall structure" width="100%" data-zoomable=""/></p> <p>Upon creating the trainer, you must pass in an SFT model. The trainer clones it into a reference model (an unupdated SFT) and a policy model, either <code class="language-plaintext highlighter-rouge">AutoModelForCausalLMWithValueHead</code> or <code class="language-plaintext highlighter-rouge">AutoModelForSeq2SeqLMWithValueHead</code>. This Value Head acts as the Value Model.</p> <p>Before entering the step, query and response embeddings are fed in batches, along with the reward for the response. You need to:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">ppo_trainer</span><span class="p">.</span><span class="n">dataloader</span><span class="p">:</span>
	<span class="n">query_tensors</span> <span class="o">=</span> <span class="nf">get_query_tensors</span><span class="p">()</span>
	<span class="n">response_tensors</span> <span class="o">=</span> <span class="nf">generate_response_tensors_given_query_with_policy_LM</span><span class="p">()</span>
	<span class="n">rewards</span> <span class="o">=</span> <span class="nf">get_reward_score_for_the_response</span><span class="p">()</span>
	<span class="n">stats</span> <span class="o">=</span> <span class="n">ppo_trainer</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">query_tensors</span><span class="p">,</span> <span class="n">response_tensors</span><span class="p">,</span> <span class="n">rewards</span><span class="p">)</span>
</code></pre></div></div> <p>Inside the <code class="language-plaintext highlighter-rouge">step</code> function, the process begins with two significant operations around lines 665 and 680:</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914141141.png" alt="Two significant operations" width="100%" data-zoomable=""/></p> <p>These operations involve model forward passes and reward computations, as highlighted below:</p> <h3 id="how-to-do-model-forward">How to Do Model Forward</h3> <p>The <code class="language-plaintext highlighter-rouge">batched_forward_pass()</code> function calculates the RL model’s policy probabilities \(\pi_{\theta_{\text{old}}}^{\text{RL}}(y_{w}\vert x)\) (<code class="language-plaintext highlighter-rouge">all_logprobs</code>), the SFT model’s policy probabilities \(\pi^{\text{SFT}}(y_{w}\vert x)\) (<code class="language-plaintext highlighter-rouge">ref_logprobs</code>), and the values \(V(x)\) (<code class="language-plaintext highlighter-rouge">values</code>).</p> <p><a href="https://github.com/huggingface/trl/blob/v0.7.1/trl/trainer/ppo_trainer.py#L906">trl/trl/trainer/ppo_trainer.py at v0.7.1 · huggingface/trl (github.com)</a></p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimg20230914142459.png" alt="batched_forward_pass function" width="100%" data-zoomable=""/> <img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914142548.png" alt="Details of batched_forward_pass" width="100%" data-zoomable=""/></p> <p>The <code class="language-plaintext highlighter-rouge">logprobs</code> function calculates the probabilities based on the logits, which represent the scores for all tokens in the vocabulary at each position in the sequence. After applying softmax, the probabilities of the tokens appearing in the response are determined. The comparison involves decoder input from index 1 and decoder output from 0 to the second-to-last token.</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914142734.png" alt="Logprobs calculation" width="100%" data-zoomable=""/></p> <p>Additionally, a mask (Line 959) is applied to ignore the first token and calculate values only for tokens from index 1 to before the padding token.</p> <p>In Line 946, <code class="language-plaintext highlighter-rouge">logits, _, values = model(**input_kwargs)</code>, what is the meaning of <code class="language-plaintext highlighter-rouge">values</code> here? Does the <code class="language-plaintext highlighter-rouge">forward</code> function in the HuggingFace have this return value? It turns out that TRL uses <code class="language-plaintext highlighter-rouge">LMWithValueHead</code> here, letting the model have a <code class="language-plaintext highlighter-rouge">value_head</code> attribute, which is a linear layer.</p> <h3 id="value-head">Value Head</h3> <p>The value head is shared with the policy model’s underlying parameters but swaps the top layer for a linear model.</p> <p><a href="https://github.com/huggingface/trl/blob/v0.7.1/trl/models/modeling_value_head.py#L260">trl/trl/models/modeling_value_head.py at v0.7.1 · huggingface/trl (github.com)</a></p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913102810.png" alt="Value Head structure" width="100%" data-zoomable=""/></p> <p>The <code class="language-plaintext highlighter-rouge">self.v_head()</code> function requires the last hidden state to predict the value.</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913103215.png" alt="Value Head operation" width="100%" data-zoomable=""/></p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230913103109.png" alt="Further details on Value Head" width="100%" data-zoomable=""/></p> <h3 id="reward-1">Reward</h3> <p>The <code class="language-plaintext highlighter-rouge">compute_reward()</code> function is crucial for calculating the rewards based on the policy probabilities and SFT probabilities computed earlier.</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914143435.png" alt="compute_reward function" width="100%" data-zoomable=""/></p> <p>The rewards are calculated for each token treated as an action, computing \(r−\beta×KL\), with \(r\) having a value only for the last token, as per design.</p> <h3 id="advantage-gae">Advantage (GAE)</h3> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914144039.png" alt="Advantage (GAE) calculation" width="100%" data-zoomable=""/></p> <p>The Generalized Advantage Estimation (GAE) calculates values, advantages, and returns, incorporating a mask for calculation.</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914144124.png" alt="Details on GAE calculation" width="100%" data-zoomable=""/></p> <h3 id="ppo-update">PPO Update</h3> <p>All data is stored as old outputs before iterating over minibatches for <code class="language-plaintext highlighter-rouge">self.config.ppo_epochs</code> epochs defined by <code class="language-plaintext highlighter-rouge">self.config.ppo_epochs</code>.</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914144241.png" alt="PPO Update preparation" width="100%" data-zoomable=""/> <img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914144403.png" alt="Minibatch iteration" width="100%" data-zoomable=""/></p> <p>The minibatch dictionary <code class="language-plaintext highlighter-rouge">mini_batch_dict</code> calculates the information before the PPO update.</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimg20230914144537.png" alt="Minibatch information" width="100%" data-zoomable=""/></p> <p>The training of minibatches combines the Policy and Value losses for joint updates.</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914144606.png" alt="Training minibatches" width="100%" data-zoomable=""/> <img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimg20230914144700.png" alt="Combined loss update" width="100%" data-zoomable=""/></p> <p>The loss function is specially tailored, with the value loss being clipped and averaged over unmasked values, while the policy gradient loss remains standard.</p> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/img20230914145020.png" alt="Loss function details" width="100%" data-zoomable=""/></p> <p>This comprehensive overview explains how the <code class="language-plaintext highlighter-rouge">step</code> function integrates various components of the PPO algorithm, from calculating probabilities and values to updating the policy and value models based on computed rewards and advantages, ultimately refining the RL model’s behavior towards desired outcomes.</p>]]></content><author><name></name></author><category term="TechEssays"/><category term="NLP"/><category term="LLM"/><summary type="html"><![CDATA[Tech essays of Reinforcement Learning from Human Feedback (RLHF) and Proximal Policy Optimization (PPO) with codes in Huggingface TRL.]]></summary></entry><entry><title type="html">Reading Notes of How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources</title><link href="https://wind-2375-like.github.io/blog/2023/reading-notes-chatgpt/" rel="alternate" type="text/html" title="Reading Notes of How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources"/><published>2023-02-19T12:50:37+00:00</published><updated>2023-02-19T12:50:37+00:00</updated><id>https://wind-2375-like.github.io/blog/2023/reading-notes-chatgpt</id><content type="html" xml:base="https://wind-2375-like.github.io/blog/2023/reading-notes-chatgpt/"><![CDATA[<p>Reading notes of Yao’s notes of <a href="https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1">How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources (notion.site)</a>.</p> <p><strong>Keywords:</strong> code and instruction tuning; trade of between in-context learning and instruction tuning; unlock or inject the abilities; instruction-tuning, supervised instruction-tuning, and RLHF instruction-tuning; knowledge and reasoning.</p> <h2 id="reference">Reference</h2> <blockquote> <p>Fu, Yao; Peng, Hao and Khot, Tushar. (Dec 2022). How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources. Yao Fu’s Notion. https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1</p> </blockquote> <h2 id="road-map">Road Map</h2> <p><img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimage-20230218225348976.png" alt="image-20230218225348976"/></p> <p>Note: The base model for code-davinci-002 is highly likely not be the initial GPT-3 davinci model.</p> <h2 id="summary">Summary</h2> <p>The authors’ conclusion:</p> <ul> <li>The <strong>language generation ability</strong> + <strong>basic world knowledge</strong> + <strong>in-context learning</strong> are from pretraining (<code class="language-plaintext highlighter-rouge">davinci</code>)</li> <li>The ability to <strong>store a large amount of ==knowledge==</strong> is from the 175B scale.</li> <li>The ability to <strong>follow instructions</strong> and <strong>generalizing to new tasks</strong> are from scaling instruction tuning (<code class="language-plaintext highlighter-rouge">davinci-instruct-beta</code>)</li> <li>The ability to perform <strong>complex ==reasoning==</strong> is likely to be from training on code (<code class="language-plaintext highlighter-rouge">code-davinci-002</code>)</li> <li>The ability to <strong>generate neutral, objective, safe, and informative answers</strong> are from alignment with human. Specifically: <ul> <li>If supervised tuning, the resulting model is <code class="language-plaintext highlighter-rouge">text-davinci-002</code></li> <li>If RLHF, the resulting model is <code class="language-plaintext highlighter-rouge">text-davinci-003</code></li> <li>Either supervised or RLHF, the models cannot outperform <code class="language-plaintext highlighter-rouge">code-davinci-002</code> on many tasks, which is called the <em>alignment tax</em>. (RLHF on 003 just recovers the in-context learning ability.)</li> </ul> </li> <li>The <strong>dialog ability</strong> is also from RLHF (<code class="language-plaintext highlighter-rouge">ChatGPT</code>), specifically it tradeoffs in-context learning for: <ul> <li>Modeling dialog history</li> <li>Increased informativeness</li> <li>Rejecting questions outside the model’s knowledge scope</li> </ul> </li> </ul> <table> <thead> <tr> <th>Ability</th> <th>OpenAI Model</th> <th>Training Method</th> <th>OpenAI API</th> <th>OpenAI Paper</th> <th>Open Source Approximate</th> </tr> </thead> <tbody> <tr> <td>GPT-3 Series</td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td>Generation <br/>+ World Knowledge <br/>+ In-context Learning</td> <td>GPT-3 Initial<br/><br/><strong>Many abilities already within this model, although superficially weak</strong></td> <td>Language Modeling</td> <td>Davinci</td> <td><a href="https://arxiv.org/abs/2005.14165">GPT 3 Paper</a></td> <td><a href="https://arxiv.org/abs/2205.01068">Meta OPT</a><br/><a href="https://arxiv.org/abs/2211.05100">BLOOM</a><br/><a href="https://arxiv.org/abs/2211.09085">Galactica</a></td> </tr> <tr> <td>+ Follow Human Instruction<br/>+ Generalize to unseen task (free lunch of scaling instructions)</td> <td>Instruct-GPT initial</td> <td>Instruction Tuning</td> <td>Davinci-Instruct-Beta</td> <td><a href="https://arxiv.org/abs/2203.02155">Instruct-GPT paper</a></td> <td><a href="https://arxiv.org/abs/2110.08207">T0 paper</a><br/><a href="https://arxiv.org/abs/2109.01652">Google FLAN paper</a><br/><a href="https://arxiv.org/abs/2210.11416">Google FLAN paper</a></td> </tr> <tr> <td>+ Code Understanding<br/>+ Code Generation</td> <td>Codex initial</td> <td>Training on Code</td> <td>Code-Cushman-001</td> <td><a href="https://arxiv.org/abs/2107.03374">Codex Paper</a></td> <td><a href="https://arxiv.org/abs/2203.13474">Salesforce CodeGen</a></td> </tr> <tr> <td>GPT-3.5 Series</td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td>++ Code Understanding<br/>++ Code Generation<br/>++ Complex Reasoning / Chain of Thought (<em>why</em>?)<br/>+ Long-term dependency (<em>probably</em>)</td> <td>Current Codex<br/><br/><strong>Strongest model in GPT3.5 Series</strong></td> <td>Training on text + code <br/>Tuning on instructions</td> <td>Code-Davinci-002</td> <td><a href="https://arxiv.org/abs/2107.03374">Codex Paper</a></td> <td>??</td> </tr> <tr> <td>++ Follow Human Instruction<br/>- In-context learning<br/>- Reasoning<br/>++ Zero-shot generation</td> <td>Instruct-GPT supervised<br/><br/><strong>Trade in-context learning for zero-shot generation</strong></td> <td>Supervised instruction tuning</td> <td>Text-Davinci-002</td> <td><a href="https://arxiv.org/abs/2203.02155">Instruct-GPT paper</a>, supervised part</td> <td><a href="https://arxiv.org/abs/2110.08207">T0 paper</a><br/><a href="https://arxiv.org/abs/2109.01652">Google FLAN paper</a><br/><a href="https://arxiv.org/abs/2210.11416">Google FLAN paper</a></td> </tr> <tr> <td>+ Follow human value<br/>+ More detailed generation<br/>+ In-context learning<br/>+ Zero-shot generation</td> <td>Instruct-GPT RLHF<br/><br/><strong>More aligned than 002, less performance loss</strong></td> <td>Instruction tuning w. RLHF</td> <td>Text-Davinci-003</td> <td>Instruct-GPT paper, RLHF part <a href="https://arxiv.org/abs/2009.01325">Summarization .w human feedback</a></td> <td><a href="https://arxiv.org/abs/2209.14375">DeepMind Sparrow paper</a><br/><a href="https://arxiv.org/abs/2210.01241">AI2 RL4LMs</a></td> </tr> <tr> <td>++ Follow human value<br/>++ More detailed generation<br/>++ Reject questions beyond its knowledge (why?)<br/>++ Model dialog context<br/>– In-context learning</td> <td>ChatGPT<br/><br/><strong>Trade in-context learning for dialog history modeling</strong></td> <td>Tuning on dialog w. RLHF</td> <td>-</td> <td>-</td> <td><a href="https://arxiv.org/abs/2209.14375">DeepMind Sparrow paper</a><br/><a href="https://arxiv.org/abs/2210.01241">AI2 RL4LMs</a></td> </tr> </tbody> </table> <h2 id="limitations">Limitations</h2> <ol> <li><strong>On-the-fly overwriting the model’s belief</strong>: when the model expresses its belief in something, it might be hard to correct it when the belief is wrong. There seems to be a hierarchy of how strong the belief is, which means that there exists a list of very strong core belief. ==<strong>It is extremely important to ensure such core belief should be absolutely 100% aligned with human.</strong>==</li> <li><strong>Formal reasoning</strong>: the GPT-3.5 series cannot do reasoning within formal, strict systems like math or first-order logic. <ol> <li>The word “reasoning” is less well-defined. Yet if we view there is a spectrum of ambiguity like (a) very ambiguous, no reasoning; (b) mixture of logic and ambiguous statements; (c). no ambiguity has to be very rigorous, then,</li> <li>The model can do very well on type (b) reasoning with ambiguity.</li> <li>The model cannot do type (c) reasoning, <strong>yet whether such reasoning should be done by a language model or a symbolic system is up for discussion.</strong></li> <li>Update: <a href="https://writings.stephenwolfram.com/2023/01/wolframalpha-as-the-way-to-bring-computational-knowledge-superpowers-to-chatgpt/">Wolfram Alpha as the Way to Bring Computational Knowledge Superpowers to ChatGPT—Stephen Wolfram Writings</a></li> </ol> </li> <li><strong>Retrieval from the Internet</strong>: the GPT-3.5 series cannot directly search the internet (for now) <ol> <li>Likely tested within OpenAI: <a href="https://openai.com/blog/webgpt/">WebGPT: Improving the Factual Accuracy of Language Models through Web Browsing (openai.com)</a></li> <li><strong>Separate the knowledge and reasoning</strong>: it would be ideal if we could offload the knowledge part to the outside retrieval system and let the language model only focus on reasoning.</li> <li><strong>Combining LLMs (reasoning) and search (knowledge) is a good direction</strong>: LLMs are good for reasoning, not for knowledge. The knowledge within LLMs are unreliable and cannot be verified. On the other hand, the knowledge from search engine is orders or magnitude larger than LLM’s internal knowledge, can one can easily verify credibility of search results by checking the source. <ol> <li>Retrieval-augmented models?</li> <li>Reduce the model size? (175B storing unreliable knowledge -&gt; search engines)</li> </ol> </li> </ol> </li> </ol> <h2 id="questions-to-be-answered">Questions to be answered</h2> <ol> <li>How does the GPT-3.5 acquire the reasoning ability (CoT)? (Likely because of code training according to the authors).</li> <li>Which kinds of abilities are unlocked/injected by what means<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>? The authors’ hypothesis (according to scale): <ol> <li>Abilities from code training: injection</li> <li>Abilities from instruction tuning: unlock</li> </ol> </li> </ol> <h2 id="what-i-need-to-further-know-about">What I need to further know about</h2> <ul> <li>instruction-tuning? supervised instruction-tuning? RLHF instruction-tuning?</li> <li>code training?</li> </ul> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>“by what means” is hypothesized in the <a href="##summary">summary</a> section but some of them are not proved. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="ReadingNotes"/><category term="NLP"/><category term="LLM"/><summary type="html"><![CDATA[Reading notes of Yao's notes of "How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources".]]></summary></entry><entry><title type="html">Huggingface parallel training for solving the CUDA out of memory issue</title><link href="https://wind-2375-like.github.io/blog/2023/huggingface-parallel/" rel="alternate" type="text/html" title="Huggingface parallel training for solving the CUDA out of memory issue"/><published>2023-02-12T23:37:32+00:00</published><updated>2023-02-12T23:37:32+00:00</updated><id>https://wind-2375-like.github.io/blog/2023/huggingface-parallel</id><content type="html" xml:base="https://wind-2375-like.github.io/blog/2023/huggingface-parallel/"><![CDATA[<h2 id="background">Background</h2> <p>I am running conditional text generation experiments these days on various seq2seq models. The script went well on other models except for the T5-large model. I continuously got the CUDA OOM error.</p> <p>The T5-large model is not so big and I’m running code on a server with 4 NVIDIA Corporation GP102s. Each one has 12G of video memory. It seems that I could run my code. However, even I set the batch size to 1 for each device, I got the CUDA OOM error.</p> <h2 id="environment">Environment</h2> <p>My huggingface transformer version is <code class="language-plaintext highlighter-rouge">4.20.1</code> and my code looks like <a href="https://github.com/huggingface/notebooks/blob/main/examples/summarization.ipynb">this</a> (<code class="language-plaintext highlighter-rouge">preprocess_function</code>, <code class="language-plaintext highlighter-rouge">dataset.map</code>, <code class="language-plaintext highlighter-rouge">trainer</code>).</p> <h2 id="some-tries">Some tries</h2> <p>I followed the <a href="https://github.com/huggingface/transformers/issues/9311">advice</a> and added <code class="language-plaintext highlighter-rouge">--fp16</code> and <code class="language-plaintext highlighter-rouge">--sharded_ddp</code> but neither of them work.</p> <h2 id="solution">Solution</h2> <p><a href="https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Model.parallelize">Parallelize</a></p> <p><code class="language-plaintext highlighter-rouge">( device_map = None )</code></p> <p>Parameters</p> <ul> <li><strong>device_map</strong> (<code class="language-plaintext highlighter-rouge">Dict[int, list]</code>, optional, defaults to None) — A dictionary that maps attention modules to devices. Note that the embedding module and LMHead are always automatically mapped to the first device (for esoteric reasons). That means that the first device should have fewer attention modules mapped to it than other devices. For reference, the t5 models have the following number of attention modules: <ul> <li>t5-small: 6</li> <li>t5-base: 12</li> <li>t5-large: 24</li> <li>t5-3b: 24</li> <li>t5-11b: 24</li> </ul> </li> </ul> <p>Uses a device map to distribute attention modules of the model across several devices. If no device map is given, it will evenly distribute blocks across all devices.</p> <p>Example:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Here is an example of a device map on a machine with 4 GPUs using t5-3b, which has a total of 24 attention modules:
</span><span class="n">model</span> <span class="o">=</span> <span class="n">T5ForConditionalGeneration</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">t5-3b</span><span class="sh">"</span><span class="p">)</span>
<span class="n">device_map</span> <span class="o">=</span> <span class="p">{</span>
    <span class="mi">0</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
    <span class="mi">1</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span>
    <span class="mi">2</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span>
    <span class="mi">3</span><span class="p">:</span> <span class="p">[</span><span class="mi">17</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">23</span><span class="p">],</span>
<span class="p">}</span>
<span class="n">model</span><span class="p">.</span><span class="nf">parallelize</span><span class="p">(</span><span class="n">device_map</span><span class="p">)</span>
</code></pre></div></div> <p>I just copy the code and replace “t5-3b” with “t5-large” and it works!</p>]]></content><author><name></name></author><category term="TechEssays"/><category term="NLP"/><category term="CUDA"/><summary type="html"><![CDATA[Document a workable solution for the annoying CUDA Out Of Memory (OOM).]]></summary></entry><entry><title type="html">Could you give me a hint? Generating inference graphs for defeasible reasoning</title><link href="https://wind-2375-like.github.io/blog/2023/give-me-a-hint/" rel="alternate" type="text/html" title="Could you give me a hint? Generating inference graphs for defeasible reasoning"/><published>2023-01-24T00:09:26+00:00</published><updated>2023-01-24T00:09:26+00:00</updated><id>https://wind-2375-like.github.io/blog/2023/give-me-a-hint</id><content type="html" xml:base="https://wind-2375-like.github.io/blog/2023/give-me-a-hint/"><![CDATA[<p><strong>Keywords:</strong> Inference Graphs, Transfer Learning, Defeasible Reasoning, Human Aiding.</p> <p><img alt="Fig 1" src="https://cdn.jsdelivr.net/gh/Wind2375like/I-m_Ghost@main/img/5e400103c58e8b20417827784022bd36321554b8932953bc79f3d4cd060d2dee.png" width="100%" data-zoomable=""/></p> <p>Link: <a href="https://arxiv.org/pdf/2105.05418.pdf">2105.05418.pdf (arxiv.org)</a></p> <h2 id="contribution">Contribution</h2> <ul> <li>Inference/Influence Graph for representing defeasible reasoning</li> <li>Transfer learning for generating influence graph</li> <li>Improvement of human performance with the aid of influence graph</li> </ul> <h2 id="method">Method</h2> <h3 id="inference-graph-and-influence-graph">Inference Graph and Influence Graph</h3> <p><img alt="Fig 2" src="https://cdn.jsdelivr.net/gh/Wind2375like/I-m_Ghost@main/img/aaac32bcb2ab9b58ed0f595dfab10c0f6efec1727fae8cbcdff2ed41aaaf87fe.png" width="100%" data-zoomable=""/></p> <p>A typical causal inference is <code class="language-plaintext highlighter-rouge">P -&gt; H</code>, where <code class="language-plaintext highlighter-rouge">P</code> is the premise and <code class="language-plaintext highlighter-rouge">H</code> is the hypothesis, i.e. the outcome of the premise. The arrow shows the causal relationship. Given new evidence, the hypothesis may be strengthened <code class="language-plaintext highlighter-rouge">P|U -+-&gt; H</code> or weakened <code class="language-plaintext highlighter-rouge">P|U- -x-&gt; H</code>. When it is weakened, this kind of reasoning is called defeasible reasoning. We can treat <code class="language-plaintext highlighter-rouge">U</code> as “assumptions” and <code class="language-plaintext highlighter-rouge">U-</code> as “defeaters”.</p> <p>Then the paper adopted an inference graph (left). Added contextualizers (something related to premises) and mediators (deduction step from <code class="language-plaintext highlighter-rouge">U</code> to <code class="language-plaintext highlighter-rouge">H</code>). I think it is the dataset WIQA’s contribution (the two works share the same author) instead of the paper. But anyway, the paper switched its context from inference graph to influence graph (right), which is directly from WIQA.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://cdn.jsdelivr.net/gh/Wind2375like/I-m_Ghost@main/img/f36f9ba147a9e2566ea2f9705f2e7b4f06bcabc54772063a67ad538feb065a9b-480.webp 480w,https://cdn.jsdelivr.net/gh/Wind2375like/I-m_Ghost@main/img/f36f9ba147a9e2566ea2f9705f2e7b4f06bcabc54772063a67ad538feb065a9b-800.webp 800w,https://cdn.jsdelivr.net/gh/Wind2375like/I-m_Ghost@main/img/f36f9ba147a9e2566ea2f9705f2e7b4f06bcabc54772063a67ad538feb065a9b-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="https://cdn.jsdelivr.net/gh/Wind2375like/I-m_Ghost@main/img/f36f9ba147a9e2566ea2f9705f2e7b4f06bcabc54772063a67ad538feb065a9b.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://cdn.jsdelivr.net/gh/Wind2375like/I-m_Ghost@main/img/24ebec2264b5bf120a465ebce58e4fb71b9800c8f8961b36c594459648f31619-480.webp 480w,https://cdn.jsdelivr.net/gh/Wind2375like/I-m_Ghost@main/img/24ebec2264b5bf120a465ebce58e4fb71b9800c8f8961b36c594459648f31619-800.webp 800w,https://cdn.jsdelivr.net/gh/Wind2375like/I-m_Ghost@main/img/24ebec2264b5bf120a465ebce58e4fb71b9800c8f8961b36c594459648f31619-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="https://cdn.jsdelivr.net/gh/Wind2375like/I-m_Ghost@main/img/24ebec2264b5bf120a465ebce58e4fb71b9800c8f8961b36c594459648f31619.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Green arrow: positive effect. Red arrow: negative effect. </div> <h3 id="transfer-learning">Transfer Learning</h3> <p>Transfer learning refers to the task when you want to let your model perform a task on one dataset, but it doesn’t have any label. So you find another similar dataset with labels and adjust it to the same format as your intended task/dataset. Then you train your model on this similar dataset and then do the prediction on your target dataset.</p> <p>Since the domain in the test set has changed, a good performance in the test set will indicate not only good accuracy but also excellent generalizability.</p> <p>The paper aims to generate an influence graph given <code class="language-plaintext highlighter-rouge">P</code>, <code class="language-plaintext highlighter-rouge">H</code>, and <code class="language-plaintext highlighter-rouge">U</code>. It uses seq2seq architecture. The input of the encoder is easy, which is:</p> <p><img alt="Fig 5" src="https://cdn.jsdelivr.net/gh/Wind2375like/I-m_Ghost@main/img/0b16ab260bf0ce6c2a55d66a46a9991ccd11eee92253b84a7d73dcb479432bf8.png" width="100%" data-zoomable=""/></p> <p>The output of the decoder is the sequential representation of the influential graph. See the appendix for details.</p> <p><img alt="Fig 6" src="https://cdn.jsdelivr.net/gh/Wind2375like/I-m_Ghost@main/img/8a03da0d2851efa544057266c1382199a4b9d3c0e046a6447098927ce7843a07.png" width="50%" data-zoomable=""/></p> <p>The authors train the models (GPT2, T5) on WIQA and generate these graphs on SNLI, SOCIAL, and ATOMIC.</p> <p>They did a train-test split to WIQA and evaluated their model.</p> <p><img alt="Fig 7" src="https://cdn.jsdelivr.net/gh/Wind2375like/I-m_Ghost@main/img/90d230c07462707fc530967d3b99a94104cea66b26a27d02879ad817c01eaa5c.png" width="50%" data-zoomable=""/></p> <h3 id="human-improvement">Human Improvement</h3> <p>Then they observed if the graph helps people. They sampled a challenging subset.</p> <p>&lt;img alt=”Fig 8” src=”https://cdn.jsdelivr.net/gh/Wind2375like/I-m_Ghost@main/img/91c77c1c460f40ad46021f486a00c1e3b526bd43551bceda704ae511489b4a67.png” width=”50%” data-zoomable”/&gt;</p> <p>There is some improvement.</p>]]></content><author><name></name></author><category term="ReadingNotes"/><category term="NLP"/><category term="CausalReasoning"/><summary type="html"><![CDATA[A reading note about a paper related to defeasible reasoning.]]></summary></entry></feed>